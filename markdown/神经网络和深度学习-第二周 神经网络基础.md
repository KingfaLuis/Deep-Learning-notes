---
typora-copy-images-to: img
typora-root-url: img
---

# 神经网络和深度学习--神经网络基础

[TOC]



## 2-1 二分分类



![1536412669641](C:\Users\my\AppData\Local\Temp\1536412669641.png)



对于二分类问题，大牛给出了一个小的Notation。

- 样本：(x,y)，训练样本包含m个；
- 其中x∈Rnx，表示样本x 包含nx个特征；
- y∈0,1，目标值属于0、1分类；
- 训练数据：{(x(1),y(1)),(x(2),y(2)),⋯,(x(m),y(m))}

 

![1536412691708](C:\Users\my\AppData\Local\Temp\1536412691708.png)



## 2.2 逻辑回归		

逻辑回归是一个用于监督学习问题的学习算法，输出值不是0就是1.

逻辑回归中用到的参数：

![1536413883281](/1536413883281.png)

sigmold函数的图像

![1536414058815](/1536414058815.png)

- 如果z趋向于比较大的正数，那么sigmold等于1。
- 如果z趋向于比较大的负数，那么sigmold等于0。
- 如果z等于零，那么sigmold等于0.5。



![1536413449443](/1536413449443.png)											





逻辑回归代价函数

![1536413558267](/1536413558267.png)



## 2.3 逻辑回归损失函数	

损失函数的公式

![1536414615223](/1536414615223.png)



但是，对于logistic regression 来说，一般不适用平方错误来作为Loss Function，这是因为上面的平方错误损失函数一般是非凸函数（non-convex），其在使用低度下降算法的时候，容易得到局部最优解，而不是全局最优解。因此要选择凸函数。 

- 我们的目标是最小化样本点的损失Loss Function，损失函数是针对单个样本点的。

对于m个样本的逻辑回归的代价函数



![1536414785380](/1536414785380.png)

代价函数是损失函数的平均值。



## 2.4 梯度下降法

用梯度下降法（Gradient Descent）算法来最小化Cost function，以计算出合适的w和b的值。 

![1536414931874](/1536414931874.png)





![1536414950421](/1536414950421.png)

每次迭代更新的修正表达式： 

![1536415203169](/1536415203169.png)

在程序代码中，我们通常使用 

![1536415174254](/1536415174254.png)



## 2.5 基本的神经网络编程--导数

理解导数

导数的直观理解就是经过某点的直线的斜率

![1536415350663](/1536415350663.png)



## 2.6 更过导数的例子



![1536415479842](/1536415479842.png)

**更多倒数的例子**



![1536415526725](/1536415526725.png)



## 2.7 计算图



![1536415705359](/1536415705359.png)

链式法则求导

![1536415750378](/1536415750378.png)



## 2.9 逻辑回归中的梯度下降法

逻辑回归损失函数回顾

![1536415941385](/1536415941385.png)

逻辑回归求导

![1536415981408](/1536415981408.png)



## 2-10 m个样本的梯度下降法

![1536416260835](/1536416260835.png)

对m个样本来说，其Cost function表达式如下： 

![1536416200327](/1536416200327.png)



Cost function 关于w和b的偏导数可以写成所有样本点偏导数和的平均形式： 

![1536416230368](/1536416230368.png)

 ## 2.14 向量化logistics回归的梯度输出	





在深度学习的算法中，我们通常拥有大量的数据，在程序的编写过程中，应该尽最大可能的少使用loop循环语句，利用python可以实现矩阵运算，进而来提高程序的运行速度，避免for循环的使用。 

什么是向量化？				

![1536416673972](/1536416673972.png)

#### 逻辑回归向量化

![1536416906211](/1536416906211.png)



所有m个样本的线性输出Z可以用矩阵表示： 

![1536417032823](/1536417032823.png)

python 代码

```python
Z = np.dot(w,T,X) + b
A = sigold(Z)
```

## 2.12 更多的向量化的例子--神经网络

![1536417209587](/1536417209587.png)



![1536417225270](/1536417225270.png)



![1536417240481](/1536417240481.png)



## 2.13 向量化逻辑回归的梯度计算

![1536417439977](/1536417439977.png)





![1536417470800](/1536417470800.png)



```python
Z = np.dot(w,T,X) + b
A = sigmold(Z)
dZ = A-Y
dw = 1/m*np.dot(X,dZ.T)
db = i/m*np.sum(dZ)

w = w - alpha*dw
b = b - alpha*db

```



## 2.15 python 中的广播



![1536417814391](/1536417814391.png)





![1536417870136](/1536417870136.png)



![1536417913993](/1536417913993.png)



## 2.16 python numpy 向量的说明

- 虽然在Python有广播的机制，但是在Python程序中，为了保证矩阵运算的正确性，可以使用reshape()函数来对矩阵设定所需要进行计算的维度，这是个好的习惯；
- 如果用下列语句来定义一个向量，则这条语句生成的a的维度为（5，），既不是行向量也不是列向量，称为秩（rank）为1的array，如果对a进行转置，则会得到a本身，这在计算中会给我们带来一些问题。

![1536418123061](/1536418123061.png)



- 如果需要定义（5，1）或者（1，5）向量，要使用下面标准的语句：

```
a = np.random.randn(5,1)
b = np.random.randn(1,5)
```

- 可以使用assert语句对向量或数组的维度进行判断。assert会对内嵌语句进行判断，即判断a的维度是不是（5，1），如果不是，则程序在此处停止。使用assert语句也是一种很好的习惯，能够帮助我们及时检查、发现语句是否正确。

```
assert(a.shape == (5,1))
```

- 可以使用reshape函数对数组设定所需的维度

```
a.reshape((5,1))
```



